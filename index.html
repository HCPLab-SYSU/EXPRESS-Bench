<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Beyond the Destination: A Novel Benchmark for Exploration-Aware Embodied Question Answering"/>
  <meta property="og:description" content="Beyond the Destination: A Novel Benchmark for Exploration-Aware Embodied Question Answering"/>
  <meta property="og:url" content="EXPRESS-Bench.github.io"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>EXPRESS-Bench</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Towards Long-Horizon Vision-Language Navigation: Platform, Benchmark and Method</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
              <!-- <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Kaixuan Jiang</a><sup>1*</sup>,</span> -->
              Kaixuan Jiang</a><sup>1</sup>,</span>
                <span class="author-block">
                  <!-- <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Yang Liu</a><sup>1*</sup>,</span> -->
                  Yang Liu</a><sup>1*</sup>,</span>
                    <span class="author-block">
                      <!-- <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Weixing Chen</a><sup>1†</sup>,</span> -->
                      Weixing Chen</a><sup>1</sup>,</span>
                      <span class="author-block">
                        <!-- <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Jingzhou Luo</a><sup>2</sup>,</span> -->
                        Jingzhou Luo</a><sup>1</sup>,</span>
                        <span class="author-block">
                          <!-- <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Ziliang Chen</a><sup>2</sup>,</span> -->
                          Ziliang Chen</a><sup>2</sup>,</span>
                          <span class="author-block">
                            <!-- <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Ling Pan </a><sup>2</sup>,</span> -->
                            Ling Pan </a><sup>3</sup>,</span>
                            <span class="author-block">
                              <!-- <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Guanbin Li</a><sup>1</sup>,</span><br> -->
                              Guanbin Li</a><sup>1,2</sup>,</span>
                              <span class="author-block">
                                <!-- <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Liang Lin</a><sup>1</sup>,</span> -->
                                Liang Lin</a><sup>1,2</sup></span>
                  <!-- </span> -->
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Sun Yat-sen University</span>
                    <span class="author-block"><sup>2</sup>Pengcheng Laboratory</span>
                    <span class="author-block"><sup>3</sup>Hong Kong University of Science and Technology</span>
                    <span class="eql-cntrb"><small><sup>*</sup>Corresponding Author</small></span>
                    <!-- <span class="eql-cntrb"><small><sup>‡</sup>Corresponding Author</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2503.11117" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> 

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/HCPLab-SYSU/EXPRESS-Bench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Github</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2503.11117" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
  

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Embodied Question Answering (EQA) is a challenging task in embodied intelligence that requires agents to dynamically explore 3D environments, actively gather visual information, and perform multi-step reasoning to answer questions. However, current EQA approaches suffer from critical limitations in exploration efficiency, dataset design, and evaluation metrics. Moreover, existing datasets often introduce biases or prior knowledge, leading to disembodied reasoning, while frontier-based exploration strategies struggle in cluttered environments and fail to ensure fine-grained exploration of task-relevant areas. To address these challenges, we construct the EXPloration-awaRe Embodied queStion anSwering Benchmark (EXPRESS-Bench), the largest dataset designed specifically to evaluate both exploration and reasoning capabilities. EXPRESS-Bench consists of 777 exploration trajectories and 2,044 question-trajectory pairs. To improve exploration efficiency, we propose Fine-EQA, a hybrid exploration model that integrates frontier-based and goal-oriented navigation to guide agents toward task-relevant regions more effectively. Additionally, we introduce a novel evaluation metric, Exploration-Answer Consistency (EAC), which ensures faithful assessment by measuring the alignment between answer grounding and exploration reliability. Extensive experimental comparisons with state-of-the-art EQA models demonstrate the effectiveness of our EXPRESS-Bench in advancing embodied exploration and question reasoning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section" id="Pipeline">
  <div class="container is-max-desktop content">
    <h2 class="title">EXPRESS-Bench</h2>
  </div>
</section>

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/dataset.png" alt="MY ALT TEXT"/>
      <!-- <h1 class="subtitle has-text-centered"> -->
        <b>Comparison of our EXPRESS-Bench with other EQA datasets. The orange trajectory in the top-down map shows a complete exploration path from EXPRESS-Bench, with observation images at key waypoints (top-right). Data for this path is in the orange box. The blue trajectory simulates OpenEQA's episodic memory, passing near the target but not ending there. The yellow box simulates how multiple-choice data is generated in HM-EQA, lacking the exploration path. For each question, answers are based on visual observations at the endpoint, scored according to each dataset's evaluation method. Unlike HM-EQA and OpenEQA, which may give higher scores based on answer similarity, EXPRESS-Bench adjusts scores for incorrect or fabricated answers by grounding them in the agent's observations.
        <!-- </h1> -->
    </div>
  </div>
</section>
<!-- End teaser image -->


<!-- <section class="section" id="exp1">
  <div class="container is-max-desktop content">
    <h2 class="title">Navgen: an automated data generation platform</h2>
  </div>
</section> -->

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="video-compare-container" id="seq25_compare_videoDiv" style="display: inline;">
        <video class="video" id="seq25_compare_video" loop="" playsinline="" autoplay="" muted="" src="static/videos/seq_25_combine.mp4" onplay="resizeAndPlay(this)" style="height: 0px;">
        </video>
        <canvas height="1072" class="videoMerge" id="seq25_compare_videoMerge" width="1600"></canvas>
      </div>
    </div>
  </div>
</section> -->

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/frame.png" alt="MY ALT TEXT"/>
      <!-- <h1 class="subtitle has-text-centered"> -->
        The construction process of EXPRESS-Bench.
      <!-- </h1> -->
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/statistics.png" alt="MY ALT TEXT"/>
      <!-- <h1 class="subtitle has-text-centered"> -->
        Overview of the EXPRESS-Bench statistics.
      <!-- </h1> -->
    </div>
  </div>
</section>
<!-- End teaser image -->

<section class="section" id="exp2">
  <div class="container is-max-desktop content">
    <h2 class="title">Model</h2>
  </div>
</section>

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/model.png" alt="MY ALT TEXT"/>
      <!-- <h1 class="subtitle has-text-centered"> -->
        The Fine-EQA framework operates as follows: The agent initially performs coarse-grained exploration using a frontier-based strategy, then switches to goal-oriented fine-grained exploration once task-relevant regions are identified. A maximum exploration limit per region prevents excessive searching, prompting the agent to either return to frontier-based exploration or focus on the next most promising region. Throughout this process, the VLM continuously evaluates the relevance and completeness of the acquired information, guiding the agent's decision to either continue exploration or generate answers based on the most recent visual inputs, as detailed in the Appendix.
      <!-- </h1> -->
    </div>
  </div>
</section>
<!-- End teaser image -->



<section class="section" id="novel_scene">
  <div class="container is-max-desktop content">
    <h2 class="title">Experiments</h2>
  </div>
</section>

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/exp1.png" alt="MY ALT TEXT"/>
      <!-- <h1 class="subtitle has-text-centered"> -->
        Visualization of a successful long-horizon navigation of our MGDM. We highlight aligned landmarks by colored bounding boxes in images and words in the instruction using the same color. In the first navigation segment, the agent looks for a towel in the bathroom. It successfully finds both the bathroom and the towel but does not enter the bathroom or gets close enough to the towel for the task to be marked as successful. In the next phase, the agent successfully finds the box in the living room.
      <!-- </h1> -->
    </div>
  </div>
</section>
<!-- End teaser image -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{EXPRESSBench,
                  title={Beyond the Destination: A Novel Benchmark for Exploration-Aware Embodied Question Answering},
                  author={Jiang, Kaixuan and Liu, Yang and Chen, Weixing and Luo, Jingzhou and Chen, Ziliang and Pan, Ling and Li, Guanbin and Lin, Liang},
                  year={2025}
                  journal={arXiv preprint arXiv:2503.11117}
                }
                </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>